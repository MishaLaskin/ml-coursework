{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gcloud Setup\n",
    "\n",
    "To keep sessions alive for long periods of time:\n",
    "\n",
    "`gcloud compute ssh mlrl-vm --ssh-flag=\"-ServerAliveInterval=30\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1 - K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pkl to dict\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict_ = pickle.load(fo, encoding='bytes')\n",
    "    return dict_\n",
    "\n",
    "cifar_pwd = '../data/cifar-10-batches-py/data_batch_1'\n",
    "cifar_data = unpickle(cifar_pwd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into Train / Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "# split to test and train \n",
    "\n",
    "images = cifar_data[b'data']\n",
    "labels = cifar_data[b'labels']\n",
    "\n",
    "def split_data_set(x,y,p=0.9):\n",
    "    n = x.shape[0]\n",
    "    y = np.array(y)\n",
    "    rand_indices = np.random.choice(range(n),n,replace=False)\n",
    "    i = int(p*n)\n",
    "    x_train,y_train = x[rand_indices[:i]],y[rand_indices[:i]]\n",
    "    x_test,y_test = x[rand_indices[i:]],y[rand_indices[i:]]\n",
    "    return x_train,y_train,x_test,y_test\n",
    "\n",
    "x_train,y_train,x_test,y_test = split_data_set(images,labels)\n",
    "x_train,y_train,x_val,y_val =  split_data_set(x_train,y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 20.0% N: 15\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "class NearestNeighbor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self,x,y):\n",
    "        self.x_train = x\n",
    "        self.y_train = y\n",
    "    \n",
    "    def predict(self,x,k=10,n=0):\n",
    "        n = x.shape[0] if not n else n\n",
    "        \n",
    "        y_pred = np.zeros(n,dtype=np.dtype(self.y_train[0]))\n",
    "        \n",
    "        for i in range(n):\n",
    "            distances = self.distance(self.x_train,x[i,:],'L2')\n",
    "            min_indices = np.argsort(distances)[:k]\n",
    "            k_nearest = list(self.y_train[min_indices])\n",
    "            mode = max(set(k_nearest), key=k_nearest.count)\n",
    "            y_pred[i] = mode\n",
    "        return y_pred\n",
    "        \n",
    "    def distance(self,v1,v2,measure='L1'):\n",
    "        if measure == 'L1':\n",
    "            return np.sum(np.abs(v1-v2),1)\n",
    "        elif measure == 'L2':\n",
    "            return np.sqrt(np.sum((v1-v2)**2,1))\n",
    "        else:\n",
    "            raise ValueError('Please select L1 or L2 measure')\n",
    "            \n",
    "    \n",
    "    def accuracy(self,pred,y):\n",
    "        assert len(pred) == len(y), 'pred and y must have equal lengths'\n",
    "        accuracy = sum([i==j for i,j in zip(pred,y)])/len(y)\n",
    "        print('Accuracy:',str(round(100*accuracy,2))+'%','N:',len(y))\n",
    "        return accuracy\n",
    "\n",
    "nn = NearestNeighbor()\n",
    "nn.train(x_train,y_train)\n",
    "pred = nn.predict(x_test,k=6,n=15)\n",
    "accuracy = nn.accuracy(pred,y_test[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning K as Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 1\n",
      "Accuracy: 17.0% N: 100\n",
      "====================\n",
      "K = 3\n",
      "Accuracy: 18.0% N: 100\n",
      "====================\n",
      "K = 5\n",
      "Accuracy: 15.0% N: 100\n",
      "====================\n",
      "K = 10\n",
      "Accuracy: 22.0% N: 100\n",
      "====================\n",
      "K = 20\n",
      "Accuracy: 24.0% N: 100\n",
      "====================\n",
      "K = 50\n",
      "Accuracy: 23.0% N: 100\n",
      "====================\n",
      "K = 100\n",
      "Accuracy: 20.0% N: 100\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 3, 5, 10, 20, 50, 100]:\n",
    "    pred = nn.predict(x_test,k=k,n=100)\n",
    "    print('K =',k)\n",
    "    accuracy = nn.accuracy(pred,y_test[:100])\n",
    "    print('='*20)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Fold Validation\n",
    "\n",
    "When you don't have enough data, you can perform cross-validation. As an example let N=5. So you shuffle your data, and then split it into 5 equal sized folds. Select one as validation set and run your algorithm. Repeat process 5 times, each time using a different fold as validation and return the final performance average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High Level (interesting) Thoughts on ML \n",
    "https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [1., 1., 1.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 0.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.zeros((5,3))\n",
    "x[1,:] = np.ones(3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = np.ones((50,100))\n",
    "test = np.ones((13,100))\n",
    "\n",
    "dists = np.zeros((13,50))\n",
    "train = train.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.sum(train,1).reshape(-1,1)\n",
    "y = np.sum(test,1).reshape(-1,1)\n",
    "\n",
    "dists[1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2 Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass SVM\n",
    "\n",
    "Multiclass SVMs have the following loss per data tuple $(x_i,y_i)$:\n",
    "\n",
    "$$\\sum_i L_i = \\sum_{j \\neq y_i} \\max (0,s_j - s_{y_i} + \\Delta) + \\lambda \\sum_{jk} W_{jk}^2$$\n",
    "\n",
    "where\n",
    "\n",
    "* $j$ is the number of distinct classes\n",
    "* $s_j = w_j^T x_i$ is the linear prediction\n",
    "* $w_j$ is the $j$th row of the weight matrix $W$\n",
    "* $W$ is the weight matrix with shape $(N_f,N_c + 1)$. $N_c = \\sum_j 1$ is the total number of classes, and the $+1$ is an additional term for the bias (i.e. bias trick). $N_f$ is the number of features.\n",
    "* $\\Delta$ is a margin of error term. It controls how far away a wrong prediction $s_{j \\neq y_i}$ needs to be from the correct one in order to get penalized. Usually, we set it to $1$ if there's a weight regularization term.\n",
    "* $\\lambda$ is a regularization term\n",
    "\n",
    "The final loss is the sum of the $i$ losses $L=\\sum_i L_i$.\n",
    "\n",
    "## Binary SVM\n",
    "\n",
    "A binary SVM has the loss\n",
    "\n",
    "$$L = \\sum_i \\max(0,1 - y_i w^T x_i) + \\lambda \\sum_j w_j^2$$\n",
    "\n",
    "* Note that here there is no weight matrix $W$, only one weight vector $w$ with dimention $N_f$.\n",
    "\n",
    "### Equivalence Between Binary and Multiclass SVMs when $N_C=2$ \n",
    "The binary SVM is a special case of the Multiclass SVM. You can derive equivalence in the following way:\n",
    "\n",
    "1. Parametrize $j=-1,1$ as the two binary classes.\n",
    "2. Then the weight matrix $W = [ w_{1},w_{-1}]^T$.\n",
    "3. Let $w_1 = - w_{-1}$ so now we only have one weight vector $w \\equiv w_1$.\n",
    "4. Substitute these values into the multiclass SVM for both $y_i=1$ and $y_i=-1$ cases.\n",
    "5. Noting that $\\Delta$ and $\\lambda$ are scaling factors that are inversely related, you will arrive at the binary loss written above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Model\n",
    "\n",
    "This is multiclass logistic regression. The loss for element $i$ is defined as\n",
    "\n",
    "$$L_i = -\\sum_j  \\frac{e^{s_{y_i} }}{e^{s_j}}$$\n",
    "\n",
    "Equivalently, taking the log we get $L \\rightarrow \\log L$:\n",
    "\n",
    "$$L_i = - s_{y_i}  + \\log \\sum_j {e^{s_j}}$$\n",
    "\n",
    "Since the softmax function is $\\text{softmax}(z_i) = \\sum_j \\exp(z_i) / \\exp(z_j)$ this is called the Softmax Loss. Finally the explicit form of the loss $L_i$ is \n",
    "\n",
    "$$L_i = - w_{y_i}^T x_i + C  + \\log \\sum_j {e^{w_j^T x_i + C}}$$\n",
    "\n",
    "where $C$ is a constant for numerical stability. We want to set $C$ to avoid division by zero. In practice, we set $C = - \\max (w_j^T x_i)$.\n",
    "\n",
    "## Computing the Gradient\n",
    "\n",
    "The lecture is slightly incorrect in its notation. The score is $s_j = w_j^T x$ where $w_j$ is a column vector from the matrix $W$ which has shape $(D,C)$ - where $D$ is dimension and $C$ is classes. So $w_j$ has shape $(D,)$.\n",
    "\n",
    "The gradient can be written analytically. For class $k$ it is:\n",
    "\n",
    "$$\\frac{\\partial L_i}{\\partial w_k} = - \\mathbb 1 (x_i,k=y_i) + \\frac{x_i e^{w_k^T x_i +C}}{\\sum_j e^{w_j^T x_i+C}}$$\n",
    "\n",
    "where $\\mathbb 1$ is the indicator function (1 if condition is true, 0 otherwise).\n",
    "\n",
    "## Information Theory Perspective\n",
    "\n",
    "The information entropy between two distributions $p$ and $q$ is \n",
    "\n",
    "$$ H(p,q) = \\sum_x p(x) \\log q(x)$$\n",
    "\n",
    "The softmax loss tries to minimize the information entropy between the true value $p(y_i)=1$ and the predicted value $p(j)$:\n",
    "\n",
    "$$H(p(y_i),p(j)) = H(1,f(s_j)) = \\sum_j \\log \\text{softmax}(s_j)$$\n",
    "\n",
    "Since the information entropy can also be rewritten as \n",
    "\n",
    "$$H(p,q) = H(p) + D_{KL} (p\\Vert q)$$\n",
    "\n",
    "and $H(p(y_i)) = 0$, the loss is also trying to minimize the $KL$ divergence (distance) between the real distribution $p$ and the predicted on $q$. Cool stuff!\n",
    "\n",
    "## Probabilistic Interpretation \n",
    "\n",
    "Also worth noting the probabilistic interpretation that \n",
    "\n",
    "$$P(y_i | x_i;W)  = \\text{softmax} (s_{y_i})$$\n",
    "\n",
    "By minimizing $L$ we are in fact maximizing the likelihood $P(y_i|x_i;W)$ so a softmax classifier can be interpreted as a likelihood maximization algorithm.\n",
    "\n",
    "Finally, the minimization of the regularization term $R(w)$ can be interpreted as the assumption that weights $w$ come from a Gaussian prior distribution centered around mean $0$ and are therefore penalized when they take on values far away from the mean. Also cool stuff!\n",
    "\n",
    "## Normalization trick\n",
    "\n",
    "For numerical stability, rewrite \n",
    "\n",
    "$$\\text {softmax }(z_i) = \\sum_j \\exp(z_i)/\\exp(z_j)  =\\sum_j C \\exp(z_i)/ C \\exp(z_j) = \\sum_j \\exp(z_i+\\log C)/\\exp(z_j+\\log C)$$\n",
    "\n",
    "Choose $C = \\exp(- \\max_k s_k)$ for the loss, so that the exponent in $\\exp(s_j + \\log C)$ is always $\\leq 0$, meaning that it's impossible to divide by zero in the denominator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3: Stochastic Gradient Descent\n",
    "\n",
    "Before gradient descent one could try\n",
    "\n",
    "1. Set weights randomly, train, and take best set - gets to 15% on CIFAR\n",
    "2. Perturb W by a small delta W and take the new W+delta W that increases performance most - 20% on CIFAR\n",
    "3. Compute the numerical gradient - performs well but takes a long time\n",
    "4. Do stochastic gradient descent\n",
    "\n",
    "http://cs231n.github.io/optimization-1/\n",
    "\n",
    "## Numerical Approximation\n",
    "\n",
    "Practical consideration is to use the centered difference derivative \n",
    "\n",
    "$$\\partial f(x) = [f(x+h)−f(x−h)]/2h$$\n",
    "\n",
    "This is good for reference, but the numerical gradient scales as $O(N_p)$ where $N_p$ is the number of paramters. For a 1 layers CIFAR network $N_d \\approx 30K $, which is already large and it only gets worse for more complicated networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4: Backprop Intuition\n",
    "\n",
    "Very cool lecture. Presents backprop as a circuit with gates as arithmetic operations on real-valued inputs. http://cs231n.github.io/optimization-2/\n",
    "\n",
    "Some interesting examples of gates & consequences of backprop:\n",
    "\n",
    "1. Additive gate $x+y$ - distributes gradients equally to all inputs (b.c. grad f(x) = 1 if f(x) = x).\n",
    "2. Multipication gate $x*y$ - distributes gradients inversely to inputs (e.g. if $x$ is big $y$ is small, the gradients will be distributed more to $y$ and less to $x$, because grad x f(x,y) = y if f(x,y)=xy). As a result this can lead to unintuitive consequences. Small data values will be changed more that large ones. Important to normalize.\n",
    "3. Max gate $\\max (x,y)$ - will distribute gradient to the larger of the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5: NNs Part 1\n",
    "\n",
    "About the biological analogy \n",
    "https://neurophysics.ucsd.edu/courses/physics_171/annurev.neuro.28.061604.135703.pdf\n",
    "https://www.sciencedirect.com/science/article/abs/pii/S0959438814000130\n",
    "\n",
    "## Remark\n",
    "\n",
    "1. Regularization can be interpreted as biological forgetting, since weights (synaptic strengths) are driven to zero.\n",
    "\n",
    "## Activation Functions\n",
    "\n",
    "1. Sigmoid has following undesirable properties\n",
    "    1. saturates gradients - the gradient of a sigmoid is zero near $\\sigma(x) = 0,1$, meaning that during backprop the updates may be negligible and the network stops learning.\n",
    "    2. Sigmoids are not zero centered. Since their output is always greater or equal to zero it means that gradients will be updated in one direction (all pos or all neg)\n",
    "2. Tanh \n",
    "    - same as sigmoid but without undesireable property (B) since its output ranges from -1 to 1.\n",
    "3. ReLU \n",
    "    - somewhat solves problem (A) because the gradients above zero is always 1, never saturating to zero.\n",
    "    - easy to compute both forward and backward pass\n",
    "    - can irreversibly die - the 0 threshold part can kill relu neurons\n",
    "4. Leaky ReLU\n",
    "    - solves dying ReLU problem \n",
    "5. Maxout\n",
    "    - maxout neuron outputs maximum if linear inputs https://arxiv.org/pdf/1302.4389.pdf\n",
    "\n",
    "    \n",
    "More reading\n",
    "https://arxiv.org/abs/1502.01852\n",
    "\n",
    "## Universal Function Approximators\n",
    "\n",
    "http://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf\n",
    "\n",
    "http://neuralnetworksanddeeplearning.com/chap4.html\n",
    "\n",
    "## Depth of Network\n",
    "\n",
    "https://arxiv.org/abs/1312.6184\n",
    "\n",
    "https://arxiv.org/abs/1412.6550\n",
    "\n",
    "http://www.deeplearningbook.org/contents/mlp.html\n",
    "\n",
    "## Number of Neurons\n",
    "\n",
    "The more neurons that larger the network capacity the more complex functions the network can approximate. It's almost always necessary to regularize. \n",
    "\n",
    "One interesting observation is that smaller networks are less prone to overfitting because they have less parameters. Why do we not use shallow networks with limited numbers of neurons then? \n",
    "\n",
    "Interestingly, small networks have smoother / less complex loss surfaces. You'd think that would be a good thing, but it turns out it's easy to converge to suboptimal local minima. Deep networks, on the other hand, have much noisier loss surfaces with many more local minima, but because those are localized, it's easier to get out of them.\n",
    "\n",
    "https://arxiv.org/abs/1412.0233"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6: Setting Up Neural Networks\n",
    "\n",
    "\n",
    "## Data Preprocessing \n",
    "\n",
    "### Basic Preprocessing\n",
    "\n",
    "1. Mean substraction\n",
    "    1. subtracting mean of every featyres\n",
    "    2. subtracting mean of all pixels (for images)\n",
    "2. Normalization\n",
    "    1. Divide by std, so that std=1\n",
    "    2. compress min, max data values to (-1,1)\n",
    "\n",
    "Warning - the mean should be computed only across the training data, and then also subtracted from the test and val sets.\n",
    "\n",
    "### More Advanced Preprocessing\n",
    "\n",
    "1. PCA\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "\n",
    "k = 3\n",
    "x -= x.mean(0)\n",
    "N = x.shape[0]\n",
    "cov = x.T @ x / N # covariance between ith and jth feature\n",
    "U,S,V = np.linalg.svd(cov) # singular value decomposition\n",
    "                           # U - cols of U are eigenvectors\n",
    "                           # S - singular values\n",
    "x_projected = x @ U[:,:k]  # reduces from D dimension to k\n",
    "return x_projected\n",
    "```\n",
    "\n",
    "2. Whitening - divide the reduced data by its eigenvalues\n",
    "\n",
    "```\n",
    "x_projected /= np.sqrt(S[:k]+1e-5) # S vals are the eigen vals squared\n",
    "```\n",
    "\n",
    "Warning - whitening normalizes the eigenspace, which means that noisy dimensions (where most of the data is uncorrelated with the outcome) become exaggerated.\n",
    "\n",
    "## Weight Initialization\n",
    "\n",
    "Never initialize weights to zero - then all gradients are the same and nothing is learned.\n",
    "\n",
    "Initialize weights randomly. Heuristically, initialize each weight such that\n",
    "\n",
    "`w = np.random.randn(n) / sqrt(n)`\n",
    "\n",
    "where `n` is the number of inputs. The reason is that the variance is proportional to `n` meaning that the std is proportional to `sqrt(n)`. Dividing by the std normalizes the weights and scales them to a similar distribution.\n",
    "\n",
    "http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "\n",
    "https://arxiv.org/abs/1502.01852\n",
    "\n",
    "Current recommendation is to use ReLU and to do `w = np.random.randn(n) * sqrt(2.0/n)` for both weights and biases.\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "Forces activations to take on unit Gaussian distributions at the beginning of training. Usually, you put a batch norm layer immediately after dense or conv layer.\n",
    "\n",
    "https://arxiv.org/abs/1502.03167\n",
    "\n",
    "## Regularization\n",
    "\n",
    "1. L2 reg - penalize $\\frac 1 2 \\lambda W^2$\n",
    "2. L1 reg - penalize $\\lambda |W|$\n",
    "3. Elastic net - L1+L2\n",
    "4. Max norm constraint - clip weight vectors with some upper bound e.g. $||w||_2 < C$ for some constant $C$. In practice, you do one step of gradient descent and then enforce the constraint by clipping the weights.\n",
    "5. Dropout - randomly drop neurons during forward pass and backprop. Can be interpreted as selecting sub-networks and then the master network becomes an ensemble of a bunch of subnetworks.\n",
    "    - Note 1: for predicting, nothing is dropped, but outputs are scaled by dropout probability $p$.\n",
    "    - Note 2: it's best to perform scaling by $p$ during train time to keep test time performance fast.\n",
    "    - drop out paper: http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf\n",
    "    - expository paper: http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf\n",
    "    - drop connect: sets weights randomly to zero in forward pass https://cs.nyu.edu/~wanli/dropc/\n",
    "    \n",
    "## Noise Theme\n",
    "\n",
    "Dropout is part of a general theme that adding noise during training can be good.\n",
    "\n",
    "## Loss Functions\n",
    "\n",
    "### Multiclass Classification\n",
    "\n",
    "SVM Loss \n",
    "\n",
    "$$L_i = \\sum_{j\\neq y_i} \\max(0,s_j(x_i) - s_{y_i}(x_i) +1)$$\n",
    "\n",
    "Softmax loss\n",
    "\n",
    "$$L_i = - \\log \\Big (\\frac{ \\exp (s_{y_i} (x_i))}{  \\sum_j  \\exp (s_{j} (x_i)) }\\Big )$$\n",
    "\n",
    "Problem: when $N_c >>1$ the number of classes is large. You can then use a hierarchical softmax, which represented classes as a tree.\n",
    "\n",
    "https://arxiv.org/pdf/1310.4546.pdf\n",
    "\n",
    "### Binary Classification (or Multiple Attribute)\n",
    "\n",
    "You can use the binary SVM and binary logistic loss.\n",
    "\n",
    "### Regression\n",
    "\n",
    "Use the mean squared error.\n",
    "\n",
    "Note: Regression loss is much less stable than softmax. Always best to see if you can turn regression problem into a classification (e.g. binning).\n",
    "\n",
    "One example where the L2 MSE loss fails is with outliers. Outliers can generate large L2 loss values, whereas they don't have as large of an effect on the Softmax loss.\n",
    "\n",
    "### Structured Prediction\n",
    "\n",
    "N/A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 6: Neural Networks Part 3 Learning and Evaluation\n",
    "\n",
    "## *important lecture\n",
    "\n",
    "This is a highly practical boss lecture\n",
    "\n",
    "http://cs231n.github.io/neural-networks-3/\n",
    "\n",
    "## Tricks to building good models\n",
    "\n",
    "### Gradient Check\n",
    "\n",
    "Use numerical approximations to check the gradient. Specifically use the centered derivative formula\n",
    "\n",
    "$$ \\partial_x f(x+h) = \\frac{f(x+h)-f(x-h)}{2h}$$\n",
    "\n",
    "The formula has an error of $O(h^2)$ whereas the normal derivative formula has an error pf $O(h)$.\n",
    "\n",
    "### Relative Error Comparison\n",
    "\n",
    "Since $h$ is small but the gradient can also be small, it's important to scale the error appropriately.\n",
    "\n",
    "$$ \\frac{|f_a' - f_n'|}{\\max(|f_a'|,|f_n'|)}$$\n",
    "\n",
    "### Floating Point Issues\n",
    "\n",
    "#### Use Double Precision\n",
    "\n",
    "Again, since the gradients and $h$ are both small, it's important to use double floating point precision to avoid numerical errors.\n",
    "\n",
    "#### Stay in Active Floating Point Range\n",
    "\n",
    "Read here https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html\n",
    "\n",
    "### Be Mindful of Kinks in the Objective Function\n",
    "\n",
    "Many useful functions in NNs have kinks - a discontinuous term that looks like $\\max(x,y)$ (e.g. SVM loss and ReLU). The larger the network and dataset being evaluated the more errors you may have due to kinks in the numerical approximation. The best way to debug with kink functions is to use a small dataset and small network, so that the odds of encountering a derivative near a kink are much smaller.\n",
    "\n",
    "### Use only a few datapoints\n",
    "\n",
    "Errors compound with data, so use only a few data points for debugging.\n",
    "\n",
    "### Be careful with step size $h$\n",
    "\n",
    "https://en.wikipedia.org/wiki/Numerical_differentiation\n",
    "\n",
    "### Check gradients after a few epochs\n",
    "\n",
    "You want to check gradients once the loss function is decreasing. Otherwise, you'll be checking the gradients of randomized weights, which could be equal to random numerical noise.\n",
    "\n",
    "### Make sure regularization doesn't kill data signal\n",
    "\n",
    "Either set the regularization parameter to zero or remove the regularization term for the first run, and then add it in.\n",
    "\n",
    "### Turn off non-deterministic effects such as Dropout or Augmentation\n",
    "\n",
    "You want to make sure your gradients work across the network.\n",
    "\n",
    "### Check few dimensions per parameter\n",
    "\n",
    "## Sanity Checks\n",
    "\n",
    "### Check the loss\n",
    "\n",
    "Make sure the loss is returning an initial value that makes sense.\n",
    "- e.g. CIFAR & Softmax the initial loss without training should be around $-\\log (0.1) \\approx 2.3$\n",
    "- e.g. CIFAR & SVM the initial loss without training should be $9$.\n",
    "\n",
    "### Increase Regularization\n",
    "\n",
    "This should increase the loss\n",
    "\n",
    "### Overfit on a tiny subset of data\n",
    "\n",
    "Take a small sample (say 20 data points) and train the model on it. The loss should go to zero, and the model should overfit.\n",
    "\n",
    "\n",
    "## Useful Heuristics for Training\n",
    "\n",
    "### Check loss shape\n",
    "\n",
    "### Check validation / training accuracy for overfitting\n",
    "\n",
    "### Measure ratio of update magnitudes to values\n",
    "\n",
    "The intuition here is that the values should be larger than the update magnitudes (since update magnitudes change the values incrementally), but this ratio should be roughly constant around `1e-3`.\n",
    "\n",
    "```\n",
    "value_norm = np.linalg.norm(W.ravel())\n",
    "update = lr*dW\n",
    "update_norm = np.linalg.norm(update)\n",
    "print(update_norm / value_norm) # should be around 1e-3\n",
    "```\n",
    "\n",
    "### Activation / Gradient Distributions Per Layer\n",
    "\n",
    "You want activations / gradients to be distributed randomly initially. \n",
    "\n",
    "### Plot images of features in Conv Nets\n",
    "\n",
    "Plotting the features, if they are smooth and extract visible shapes then that's a good sign you're on the right path. If the features are noisy, that might be a symptom of a bug.\n",
    "\n",
    "# Gradient Update Schemes\n",
    "\n",
    "## Vanilla GD\n",
    "\n",
    "This is simply `x += - lr * dx`\n",
    "\n",
    "## Momentum\n",
    "\n",
    "First we introduce a new variable `v`, which is initialized to zero and has a scaling paramter $\\mu =0.9$.\n",
    "\n",
    "`v += mu * v - lr * dx`\n",
    "\n",
    "And then the update rule is \n",
    "\n",
    "`x+=v`\n",
    "\n",
    "Momentum is a misnomer, the value `v` acts like friction, dampening the gradient update velocity. For directions with a consistent gradient `v` accumulates velocity. The parameter is ofen annealed from `mu=0.5 -> .99` over training.\n",
    "\n",
    "## Nesterov Momentum\n",
    "\n",
    "Similar to momentum, but instead of computing on `x` you need to compute on the vector `x_ahead = mu*v +x`. Relabeling `x_ahead -> x` we get\n",
    "\n",
    "```\n",
    "v_prev = v\n",
    "v = mu*v - lr * dx\n",
    "x += mu*v_prev + (1-mu)*v # Polyak averaging\n",
    "```\n",
    "\n",
    "https://arxiv.org/pdf/1212.0901v2.pdf\n",
    "\n",
    "See section 7.2 of \n",
    "\n",
    "http://www.cs.utoronto.ca/~ilya/pubs/ilya_sutskever_phd_thesis.pdf\n",
    "\n",
    "\n",
    "\n",
    "## Anneal learning rate\n",
    "\n",
    "1. Step decay - decrease $\\alpha$ every $t$ steps\n",
    "2. Exponential decay $\\alpha = \\alpha_0 e^{-kt}$\n",
    "3. Inverse decay $\\alpha = \\alpha_0 / (1+kt)$\n",
    "\n",
    "In practice, step decay works well.\n",
    "\n",
    "## Newtons Method\n",
    "\n",
    "Simple update rule, no learning rate\n",
    "\n",
    "$$x \\leftarrow x - [Hf(x)]^{-1} \\nabla f(x)$$\n",
    "\n",
    "$H$ is the hessian (square matrix of second order derivatives `(n_params,n_params)`. This form is clean but not practical for large data sets since `n_params` can be on the order of millions of large deep networks.\n",
    "\n",
    "https://arxiv.org/abs/1311.2115\n",
    "\n",
    "https://ai.google/research/pubs/pub40565\n",
    "\n",
    "## Adagrad\n",
    "\n",
    "Simple method to normalize gradients (keep them from getting too big or small for any parameter)\n",
    "\n",
    "```\n",
    "cache = dx**2\n",
    "update = - lr*dx / np.sqrt(cache + epsilon)\n",
    "x += update\n",
    "```\n",
    "\n",
    "## RMSProp\n",
    "\n",
    "http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n",
    "\n",
    "```\n",
    "cache = decay*cache + (1-decay)*dx**2 # moving average\n",
    "update = - lr*dx / np.sqrt(cache + epsilon)\n",
    "x += update\n",
    "```\n",
    "\n",
    "## Adam\n",
    "\n",
    "RMSProp + momentum. Simple form:\n",
    "\n",
    "```\n",
    "m = decay1*m + (1-decay1)*dx # moving average of gradient vector\n",
    "v = decay2*v + (1-decay2)*dx**2 # moving average of gradient magnitude\n",
    "update = - lr*m / np.sqrt(v + epsilon)\n",
    "x+=update\n",
    "```\n",
    "\n",
    "Full form invloves bias terms. `m,v` both start as zero and are biased to it, so we need a bias correction mechanism.\n",
    "\n",
    "```\n",
    "m = decay1*m + (1-decay1)*dx # moving average of gradient vector\n",
    "mt = m / (1+decay1**t) # bias term\n",
    "v = decay2*v + (1-decay2)*dx**2 # moving average of gradient magnitude\n",
    "vt = v / (1+decay2**t) # bias term\n",
    "update = - lr*mt / np.sqrt(vt + epsilon)\n",
    "x+=update\n",
    "```\n",
    "\n",
    "## Unit Tests \n",
    "\n",
    "https://arxiv.org/abs/1312.6055\n",
    "\n",
    "## Hyperparameter Optimization\n",
    "\n",
    "Common parameters\n",
    "\n",
    "1. Learning rate\n",
    "2. Learning decay rates\n",
    "3. Regularization strength\n",
    "\n",
    "Start with coarse and move to fine hyperparameter tuning.\n",
    "\n",
    "### Worker / Master paradigm:\n",
    "- worker samples hyperparameters randomly and starts executng\n",
    "- there can be many workers per deployment\n",
    "- master monitors worker performance and kills workers if they're not performing well\n",
    "\n",
    "### One validation fold > cross-validation\n",
    "\n",
    "When there is a lot of data it's better to keep one validation fold\n",
    "\n",
    "### Hyperparameter Range\n",
    "\n",
    "Most hyperparameters are iterated in log scale. Hyperparameters have mostly multiplicative effects, this is why it's better to use a log scale.\n",
    "\n",
    "`learning_rate = 10 ** uniform(-6, 1)`\n",
    "\n",
    "### Random grid search\n",
    "\n",
    "Random grid seach is usually easier to implement and works better than iterative grid search.\n",
    "\n",
    "http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\n",
    "\n",
    "### Bayesian Hyperparemeter search\n",
    "\n",
    "Exploration/Exploitation schemes for parameter search\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Things to check during evaluation\n",
    "\n",
    "1. Same model different initialization\n",
    "2. Use cross validation to find best parameters\n",
    "3. Running average of parameters during training\n",
    "\n",
    "### Ensembles\n",
    "\n",
    "https://www.youtube.com/watch?v=EK61htlw8hY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7: Putting it together: Minimal Neural Network Case Study \n",
    "\n",
    "All work is in assignment 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Preprocessing\n",
    "\n",
    "Preprocessing using a HOG + HSV color binning increases performance.\n",
    "\n",
    "HOG tutorial:\n",
    "https://lilianweng.github.io/lil-log/2017/10/29/object-recognition-for-dummies-part-1.html#histogram-of-oriented-gradients-hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
